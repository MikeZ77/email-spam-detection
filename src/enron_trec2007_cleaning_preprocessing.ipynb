{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e107896",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b2951bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb97eb",
   "metadata": {},
   "source": [
    "## 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a4d7cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_emails = pd.read_csv('../data/enron_spam_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bfd975f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_emails = pd.read_csv(\"../data/trec2007_spam_data_cached.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f0b661",
   "metadata": {},
   "source": [
    "## 2. Raw Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121950f8",
   "metadata": {},
   "source": [
    "### 2.1 Remove Unecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d5757705",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_emails.drop(['Message ID', 'Date'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "adc45944",
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_emails.drop(['filepath'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce326be",
   "metadata": {},
   "source": [
    "### 2.2 Drop Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f91fe1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_emails.dropna(subset = ['Message'], inplace = True)\n",
    "enron_emails.Subject.fillna('None', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "692b4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_emails.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf22181",
   "metadata": {},
   "source": [
    "### 2.3 Create a binary label encoding on Spam/Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "271a8dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_emails['target'] = enron_emails['Spam/Ham'].map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7578f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_emails['target'] = trec_emails['class'].map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8daaba",
   "metadata": {},
   "source": [
    "## 3. Cleaning Message\n",
    "\n",
    "- #### Turn words into lowercase letters\n",
    "- #### Remove numerical digits\n",
    "- #### Remove punctuation\n",
    "- #### Tokenization - split a sentence into a list of words \n",
    "- #### Remove stopwords - to remove tokens not contributing to the overall meaning of a sentence\n",
    "- #### Lemmatization - condense variations of the same word to its root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4bf0b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc0a92",
   "metadata": {},
   "source": [
    "### 3.1 Clean Message into Tokenized Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39071b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_msg_tokenize(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    tokens = re.findall('\\S+', text)\n",
    "    text = [wn.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7ba30a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_emails['clean_msg_tokens'] = enron_emails['Message'].apply(lambda x: clean_msg_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "61d3364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_emails['clean_msg_tokens'] = trec_emails['contents'].apply(lambda x: clean_msg_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9dd41f",
   "metadata": {},
   "source": [
    "### 3.2 Clean Message for N-Grams Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9f5acaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_msg(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.findall('\\S+', text)\n",
    "    text = \" \".join([wn.lemmatize(word) for word in tokens if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "587063d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_emails['cleaned_msg'] = enron_emails['Message'].apply(lambda x: clean_msg(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7bb8752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_emails['cleaned_msg'] = trec_emails['contents'].apply(lambda x: clean_msg(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4faa48",
   "metadata": {},
   "source": [
    "### 3.3 Remove Extra Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "15eb3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define extra stopwords\n",
    "# https://github.com/kavgan/stop-words/blob/master/terrier-stop.txt\n",
    "extra_stopwords = ['c', 'r', 'u', 'let', 'get', 'would', 'please', 'may', 'also', \\\n",
    "                   'like', 'thanks', 'within', 'go', 'inc', 'make', 'could', 'want', \\\n",
    "                   'need', 'new', 'know', 'best', 'e', 'j', 'p', 'b', 'de', 'see', \\\n",
    "                   'take', 'made', 'ect', 'hou', 'com', 'recipient', 'to', 'cc', 'subject', \\\n",
    "                   'http','from','sent', 'fwd', 'www', 'sara', 'shackleton', 'germani', \\\n",
    "                   'sshacklensf', 'cgermannsf', 'also', 'x', 'px', 'utc', 'rev', 'char', \\\n",
    "                   'listhttpsstatethzchmailmanlistinforhelpplease', 'much', 'dont', \\\n",
    "                   'available', 'said']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "215b009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_emails['clean_msg_tokens'] = enron_emails['clean_msg_tokens'].apply(lambda x: [word for word in x if word not in extra_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d722a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_emails['clean_msg_tokens'] = trec_emails['clean_msg_tokens'].apply(lambda x: [word for word in x if word not in extra_stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca54d91",
   "metadata": {},
   "source": [
    "### 3.4 Merge Tokenized Words into Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2ab6eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_msg_rm_lst(msg_tokens):\n",
    "    \n",
    "    text = ' '.join(msg_tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1f369a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_emails['clean_msg_no_lst'] = enron_emails['clean_msg_tokens'].apply(lambda x: clean_msg_rm_lst(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "73e5115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_emails['clean_msg_no_lst'] = trec_emails['clean_msg_tokens'].apply(lambda x: clean_msg_rm_lst(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3c918",
   "metadata": {},
   "source": [
    "### 3.5 Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "13d72728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/6081008/dump-a-numpy-array-into-a-csv-file\n",
    "# https://stackoverflow.com/questions/16923281/writing-a-pandas-dataframe-to-csv-file\n",
    "enron_emails.to_csv(\"../data/enron_emails_processed1.csv\", encoding='utf-8', index = None)\n",
    "trec_emails.to_csv(\"../data/trec_emails_processed.csv\", encoding='utf-8', index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
